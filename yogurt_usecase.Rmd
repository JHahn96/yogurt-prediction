---
title: "Data Science in Healthcare"
author: "Natalie Borter, Pascal Humbel, Jonathan Hahn"
date: "18.01.2021"
output: html_document;
        pdf_document
params:
  wd: !r getwd()
---

```{r setup, include=True, cache=TRUE}
# Setting working directory to the directory of the file
knitr::opts_knit$set(root.dir = getwd())
source("collecting_data.R")
## executing data preparation with function load_dada_frame() defined in collecting_data.R
## Kannst du mir überall noch die Variablen MClassi und Rest einbinden? Ich mache sie mit ##Descriptives. Es handelt sich um eine Zusammenfassung der Daten, mal schauen ob sich das besser vorhersagen lässt???

##source("Deskriptives.R")



# define the keywords for google trend analysis
keyword.vec<-c("Joghurt", "gesund essen")

# union of all yogurts
#df<-load.data.frame(keyword.vec)

# list of all yogurts 
dfs<-load.data.frames(keyword.vec)

gesund<-dfs[[1]]$gesund.essen
Joghurt<-dfs[[1]]$Joghurt

#nur jeden vierten Eintrag auswählen
gesund<-gesund[seq(1,length(gesund),4)]
Joghurt<-Joghurt[seq(1,length(Joghurt),4)]


ntest <- 12 # measurements months, wollen wir hier wirklich nur 3 Monate?
len <- length(gesund)
trainingg<-gesund[c(1:(len-ntest))]
testingg<-gesund[c((len-ntest+1):len)]

trainingJ<-Joghurt[c(1:(len-ntest))]
testingJ<-Joghurt[c((len-ntest+1):len)]
library(forecast)
auto.arima(trainingg, ic = "aic") #nichts
auto.arima(trainingJ, ic = "aic") #1/1/1

Jog<-arima(x=trainingJ,order=c(1,1,1))
force.pred <- predict(Jog, n.ahead = 12)

force.pred
testingJ
dfs[1]
```

###Describe the dependent variable: waiting times and some of the key predictors


### Prepare the data for analysis

```{r prepare_data, warning=FALSE, message=FALSE}
library(tidyverse)
df_final<-dfs[[1]]

# create lockdown
ld_start = as.Date("08.04.2020", format="%d.%m.%Y")
ld_end = as.Date("26.04.2020", format="%d.%m.%Y")
ld<-data.frame("Date" = seq(ld_start,ld_end, by = 'days'), row.names = )
ld[,"Bezeichnung"]<-"Lockdown"

#events<-rbind(ld, hG_factoreneva)
events <- ld

h<-data_frame(
  holiday = events[,"Bezeichnung"],
  ds = as.Date(events[,"Date"]),
  lower_window = 0,
  upper_window = 1
)

# refactor columns for prophet package
A<-df_final[,c("yrwk_start", "sales")]
names(A)<-c("ds","y")
```

### Compare different models
For comparison of the models
https://www.otexts.org/fpp/2/5


```{r rmse_function}
##define two functions to assess model fit

## rmse = root mean squared error
rmse<-function(actual, predicted){
  round((sum((actual-predicted)^2)/length(actual))^.5,2)
}
## mean absolute percentage error
mape<-function(actual, predicted){
  round(mean(100*abs((actual-predicted)/actual)),2)
}
```

Partition the data into train and test
```{r prepare_data_for_prophet}
AL<-df_final[,c("yrwk_start", 
                "sales",
                "year", 
                "month", 
                "week", 
                "promo_01", 
                "promo_02", 
                "promo_03", 
                "promo_04", 
                "promo_05",
                "Joghurt",
                "gesund.essen",
                "prod.milchpreis.a.y",
                "kon.yougurtpr.180.fru.y")]
names(AL)<-c("ds",
             "y",
             "year",
             "month",
             "week",
             "promo_01",
             "promo_02",
             "promo_03",
             "promo_04",
             "promo_05",
             "Joghurt",
             "gesund.essen",
             "milchpreis",
             "yogurtpreis")

## the test set is the last 4 weeks measured, the training set is everithing else
ntest <- 12 # measurements weeks ntest definiere ich weiter oben
len <- nrow(AL)
training<-AL[1:(len-ntest),]
testing<-AL[(len-ntest+1):len,]

c(min(training$ds), max(training$ds))
c(min(testing$ds), max(testing$ds))
```

## Regression Modell
```{r fitting_mean, warning=FALSE, message=FALSE}
###hier kommt ihr Regressionsmodell rein

# Create simple Linear Regression Model
##lm.mod1 <- lm(logSales ~ week + month + promo_01, data = train[logSales > 0])
##sollen wir auch logarithmieren?
mod<-lm(y~as.numeric(week)+as.numeric(month)+as.numeric(promo_01),data=training)

testingA<-mod$coefficients[1]+mod$coefficients[2]*as.numeric(testing$week)+mod$coefficients[3]*as.numeric(testing$month)+mod$coefficients[4]*as.numeric(testing$promo_01)


training$pred_median <-mod$fitted.values
testing$pred_median <-testingA


c(rmse(training$y, training$pred_median),rmse(testing$y, testing$pred_median))
c(mape(training$y, training$pred_median),mape(testing$y, testing$pred_median))
```


## Show data types training
```{r fitting_glm, echo=TRUE, message=TRUE, warning=TRUE}
str(training)
```


## GLM: As second model to compare with, a generalized linear model with smoothness estimation is used (GLM). 
```{r fitting_glm, warning=FALSE, message=FALSE}
fit_glm<-glm(y~ds+ 
               week + 
               month +
               year +
               promo_01+
#               promo_02+
               promo_03+
               promo_04+
#               promo_05
               Joghurt+
               gesund.essen,
               data=training)

training$pred_glm <-predict(fit_glm)
testing$pred_glm <-predict(fit_glm, newdata = testing)

c(rmse(training$y, training$pred_glm),rmse(testing$y, testing$pred_glm))
c(mape(training$y, training$pred_glm),mape(testing$y, testing$pred_glm))
```


## GLM -> add poison. 
```{r fitting_glm, warning=FALSE, message=FALSE}
fit_glm<-glm(y~ds+ 
               week + 
               month +
               year +
               promo_01+
#               promo_02+
               promo_03+
               promo_04+
#               promo_05
               Joghurt+
               gesund.essen,
               family = 'poisson',
               data=training)

training$pred_glm <-predict(fit_glm)
testing$pred_glm <-predict(fit_glm, newdata = testing)

c(rmse(training$y, exp(training$pred_glm)),rmse(testing$y, exp(testing$pred_glm)))
c(mape(training$y, exp(training$pred_glm)),mape(testing$y, exp(testing$pred_glm)))
```


## GLM -> add quasipoisson. 
```{r fitting_glm, warning=FALSE, message=FALSE}
fit_glm<-glm(y~ds+ 
               week + 
               month +
               year +
               promo_01+
#               promo_02+
               promo_03+
               promo_04+
#               promo_05
               Joghurt+
               gesund.essen,
               family = 'quasipoisson',
               data=training)

training$pred_glm <-predict(fit_glm)
testing$pred_glm <-predict(fit_glm, newdata = testing)

c(rmse(training$y, exp(training$pred_glm)),rmse(testing$y, exp(testing$pred_glm)))
c(mape(training$y, exp(training$pred_glm)),mape(testing$y, exp(testing$pred_glm)))
```


## GLM -> summary
```{r fitting_glm, warning=FALSE, message=FALSE}
summary(fit_glm)
```


With summary() we checked if the continous variables have influence -> it seems not all


## GLM -> take out irrelevant variables 
```{r fitting_glm, warning=FALSE, message=FALSE}
fit_glm<-glm(y~ds+ 
               week + 
               month +
               year +
               promo_01+
#               promo_02+
               promo_03+
               promo_04,
#               promo_05+
#               Joghurt+
#               gesund.essen,
               family = 'quasipoisson',
               data=training)

training$pred_glm <-predict(fit_glm)
testing$pred_glm <-predict(fit_glm, newdata = testing)

c(rmse(training$y, exp(training$pred_glm)),rmse(testing$y, exp(testing$pred_glm)))
c(mape(training$y, exp(training$pred_glm)),mape(testing$y, exp(testing$pred_glm)))
```


## GLM -> drop1
```{r fitting_glm, warning=FALSE, message=FALSE}
drop1(fit_glm, test = 'F')
```


With drop1() we checked if the factor variables have influence -> it seems some not


## GLM -> take out irrelevant variables 
```{r fitting_glm, warning=FALSE, message=FALSE}
fit_glm<-glm(y~ds+ 
               week + 
#               month +
#               year +
               promo_01+
#               promo_02+
#               promo_03+
               promo_04,
#               promo_05+
#               Joghurt+
#               gesund.essen,
               family = 'quasipoisson',
               data=training)

training$pred_glm <-predict(fit_glm)
testing$pred_glm <-predict(fit_glm, newdata = testing)

c(rmse(training$y, exp(training$pred_glm)),rmse(testing$y, exp(testing$pred_glm)))
c(mape(training$y, exp(training$pred_glm)),mape(testing$y, exp(testing$pred_glm)))
```




## DELETE: GAM: As third model to compare with, a generalized additive model with smoothness estimation is used (GAM). 
```{r fitting_gam, warning=FALSE, message=FALSE}
library(gam)

fit_gam<-gam(y~ds+ 
               week + 
               month +
               year +
               promo_01+
#               promo_02+
               promo_03+
               promo_04+
#               promo_05
               Joghurt+
               gesund.essen
             ,data=training)

training$pred_gam <-predict(fit_gam)
testing$pred_gam <-predict(fit_gam, newdata = testing)

c(rmse(training$y, training$pred_gam),rmse(testing$y, testing$pred_gam))
c(mape(training$y, training$pred_gam),mape(testing$y, testing$pred_gam))
```

## Loop GLM and Easy Tree --> store Results in two dfs

```{r loop glm and easy tree, warning=FALSE, message=FALSE}
library(party)
# dataframe to put values of loop in
d = data.frame(prod = rep(0,19), model = rep(0,19), rmse_t=rep(0,19), rmse_test=rep(0,19), mape_t=rep(0,19), mape_test=rep(0,19))
d_glm = data.frame(prod = rep(0,19), model = rep(0,19), rmse_t=rep(0,19), rmse_test=rep(0,19), mape_t=rep(0,19), mape_test=rep(0,19))
for(i in seq(1,length(dfs))){
  #----------------------------------------------------------------------------------------------------------
  df_f<-dfs[[i]]

  # refactor columns for prophet package
  A<-df_f[,c("yrwk_start", "sales")]
  names(A)<-c("ds","y")
  #--------------------------------------------------------------------------------------------------------
  AL<-df_f[,c("yrwk_start", 
                "sales",
                "year", 
                "month", 
                "week", 
                "promo_01", 
                "promo_02", 
                "promo_03", 
                "promo_04", 
                "promo_05",
                "Joghurt",
                "gesund.essen",
                "prod.milchpreis.a.y",
                "kon.yougurtpr.180.fru.y")]
  names(AL)<-c("ds",
               "y",
               "year",
               "month",
               "week",
               "promo_01",
               "promo_02",
               "promo_03",
               "promo_04",
               "promo_05",
               "Joghurt",
               "gesund.essen",
               "milchpreis",
               "yogurtpreis")
  
  ## the test set is the last 4 weeks measured, the training set is everithing else
  ntest <- 12 # measurements weeks ntest definiere ich weiter oben
  len <- nrow(AL)
  training<-AL[1:(len-ntest),]
  testing<-AL[(len-ntest+1):len,]
  
  c(min(training$ds), max(training$ds))
  c(min(testing$ds), max(testing$ds))
  
  #--------------------------------------------------------------------------------------------------------
  # GLM - Model 1

  fit_glm<-glm(y~ds+ 
               week + 
               milchpreis +
               as.numeric(promo_01)+
               as.numeric(promo_02)+
               as.numeric(promo_03)+
               as.numeric(promo_04)+
               as.numeric(promo_05),
               data=training,
               family="quasipoisson")

  training$pred_glm <- predict(fit_glm)
  testing$pred_glm <- predict(fit_glm, newdata = testing)

  prod <- as.character(unlist(df_f[1,3]))
  model <- "model_GLM"
  a_glm <- c(rmse(training$y, exp(training$pred_glm)),rmse(testing$y, exp(testing$pred_glm)))
  b_glm <- c(mape(training$y, exp(training$pred_glm)),mape(testing$y, exp(testing$pred_glm)))
  c_glm <- c(prod,model,a_glm,b_glm)
  d_glm[i, ] = c_glm
  print(c_glm)
  
  #--------------------------------------------------------------------------------------------------------
  # tree with party
  # ds taken out -> date is not supported
  tree <- ctree(y~#ds+
                  week + 
                  month +
                  year +
                  promo_01+
                  promo_02+
                  promo_03+
                  promo_04+
                  promo_05+
                  Joghurt+
                  gesund.essen+
                  yogurtpreis+
                  milchpreis,
                  data=training)
  
  # show tree
  plot(tree)
  
  training$pred_tree <-predict(tree)
  testing$pred_tree <-predict(tree, newdata = testing)
  
  prod <- as.character(unlist(df_f[1,3]))
  model <- "model_basic_tree"
  a <- c(rmse(training$y, training$pred_tree),rmse(testing$y, testing$pred_tree))
  b <- c(mape(training$y, training$pred_tree),mape(testing$y, testing$pred_tree))
  c <- c(prod,model,a,b)
  d[i, ] = c
  print(c)
}

print(d_glm)
print(d)

```


## Easy Tree
```{r fitting_gam, echo=TRUE, message=FALSE, warning=FALSE}
library(party)
# dataframe to put values of loop in
d = data.frame(rmse_t=rep(0,19), rmse_test=rep(0,19), mape_t=rep(0,19), mape_test=rep(0,19))
for(i in seq(1,length(dfs))){
  #----------------------------------------------------------------------------------------------------------
  df_f<-dfs[[i]]

  # refactor columns for prophet package
  A<-df_f[,c("yrwk_start", "sales")]
  names(A)<-c("ds","y")
  #--------------------------------------------------------------------------------------------------------
  AL<-df_f[,c("yrwk_start", 
                "sales",
                "year", 
                "month", 
                "week", 
                "promo_01", 
                "promo_02", 
                "promo_03", 
                "promo_04", 
                "promo_05",
                "Joghurt",
                "gesund.essen",
                "prod.milchpreis.a.y",
                "kon.yougurtpr.180.fru.y")]
  names(AL)<-c("ds",
               "y",
               "year",
               "month",
               "week",
               "promo_01",
               "promo_02",
               "promo_03",
               "promo_04",
               "promo_05",
               "Joghurt",
               "gesund.essen",
               "milchpreis",
               "yogurtpreis")
  
  ## the test set is the last 4 weeks measured, the training set is everithing else
  ntest <- 12 # measurements weeks ntest definiere ich weiter oben
  len <- nrow(AL)
  training<-AL[1:(len-ntest),]
  testing<-AL[(len-ntest+1):len,]
  
  c(min(training$ds), max(training$ds))
  c(min(testing$ds), max(testing$ds))
  #--------------------------------------------------------------------------------------------------------
  # tree with party
  # ds taken out -> date is not supported
  tree <- ctree(y~#ds+
                  week + 
                  month +
                  year +
                  promo_01+
                  promo_02+
                  promo_03+
                  promo_04+
                  promo_05+
                  Joghurt+
                  gesund.essen+
                  yogurtpreis+
                  milchpreis,
                  data=training)
  
  # show tree
  plot(tree)
  
  training$pred_tree <-predict(tree)
  testing$pred_tree <-predict(tree, newdata = testing)
  
  a <- c(rmse(training$y, training$pred_tree),rmse(testing$y, testing$pred_tree))
  b <- c(mape(training$y, training$pred_tree),mape(testing$y, testing$pred_tree))
  c <- c(a,b)
  d[i, ] = c
  print(c)
}
```


## Easy Tree
```{r fitting_gam, echo=TRUE, message=FALSE, warning=FALSE}
length(dfs)
range(1:length(dfs))
a <- range(1:length(dfs))
a[1]
```


## Easy Tree
```{r fitting_gam, echo=TRUE, message=FALSE, warning=FALSE}
# tree with party
# ds taken out -> date is not supported
library(party)
tree <- ctree(y~#ds+
                week + 
                month +
                year +
                promo_01+
                promo_02+
                promo_03+
                promo_04+
                promo_05+
                Joghurt+
                gesund.essen+
                yogurtpreis+
                milchpreis,
                data=training)

# show tree
plot(tree)

training$pred_tree <-predict(tree)
testing$pred_tree <-predict(tree, newdata = testing)

c(rmse(training$y, training$pred_tree),rmse(testing$y, testing$pred_tree))
c(mape(training$y, training$pred_tree),mape(testing$y, testing$pred_tree))

```
## GLM -> Final model
```{r fitting_glm final, warning=FALSE, message=FALSE}
fit_glm<-glm(y~ds+ 
               week + 
               milchpreis +
               as.numeric(promo_01)+
               as.numeric(promo_02)+
               as.numeric(promo_03)+
               as.numeric(promo_04)+
               as.numeric(promo_05),
               data=training,
               family="quasipoisson")

training$pred_glm <- exp(predict(fit_glm))
testing$pred_glm <- exp(predict(fit_glm, newdata = testing))

c(rmse(training$y, exp(training$pred_glm)),rmse(testing$y, exp(testing$pred_glm)))
c(mape(training$y, exp(training$pred_glm)),mape(testing$y, exp(testing$pred_glm)))
library(performance)
model_performance(fit_glm)

drop1(fit_glm, test = 'F')
```

## XGBoost
```{r fitting_gam, echo=TRUE, message=FALSE, warning=FALSE}
library(xgboost)
##We need to provide the x and y variables 
## separatetly and in the following format:

training
X <- AL%>%
  select(-y, -ds)%>% 
  mutate(across(where(is.factor), as.numeric))%>%
  as.matrix(.)

X_train <- X[1:(len-ntest),]

y <- AL[1:(len-ntest),]%>%
  select(y) %>%
  as.matrix(.)

##Train model
fit.gbt <- xgboost(data = X_train, 
                   label = y,
                   nrounds=70, 
                   objective = "count:poisson")

##Make predictions on training data
pred_gbt_train = predict(fit.gbt, X[1:(len-ntest),])
##Make predictions on test data
pred_gbt_test = predict(fit.gbt, X[(len-ntest+1):len,])

##Which variables are important?
#require(ggplot2)
library(Ckmeans.1d.dp)
importance=xgb.importance(colnames(X), model = fit.gbt)
xgb.ggplot.importance(importance)
```
Check the accuracy of the model
```{r prophet_with_holydays_and_regressors_prediction_accuracy}
training$xgboost<-pred_gbt_train
testing$xgboost<-pred_gbt_test

c(rmse(training$y, training$xgboost),rmse(testing$y, testing$xgboost))
c(mape(training$y, training$xgboost),mape(testing$y, testing$xgboost))
```


## Prophet: As third model prophet ist used, first only with holidays added
```{r install_library, warning=FALSE, message=FALSE}
library(prophet)
```
```{r prophet_with_holyday_model, cache=TRUE}
m <- prophet(holidays=h, mcmc_samples=300, 
             holidays_prior_scale=0.5, 
             changepoint_prior_scale=0.01, 
             yearly.seasonality=TRUE,
             weekly.seasonality=TRUE, 
             daily.seasonality=TRUE)
m <- add_country_holidays(m, country_name = 'CH')
m <- fit.prophet(m, training)
```

Do a forecast for the model with holidays
```{r prophet_with_holydays_prediction, cache=TRUE}

future <- make_future_dataframe(m, periods = ntest, freq = 60 *60, include_history = TRUE)

fcst <- predict(m, future)
```

First visual inspection
```{r prophet_with_holydays_visual, cache=TRUE}
plot(m, fcst) + add_changepoints_to_plot(m)
prophet_plot_components(m,fcst)
```

Check the accuracy of the models
```{r prophet_with_holydays_prediction_accuracy}
n<-nrow(training)

training$yhat<-fcst$yhat[1:n]
testing$yhat<-fcst$yhat[(n+1):(n+ntest)]

c(rmse(training$y, training$yhat),rmse(testing$y, testing$yhat))
c(mape(training$y, training$yhat),mape(testing$y, testing$yhat))
```


### Second Prophet Model, this time with both, regressors and holidays


```{r prophet_with_holydays_model, cache=TRUE}
m_h_r <- prophet(holidays=h, mcmc_samples=300, 
             holidays_prior_scale=0.5, 
             changepoint_prior_scale=0.01, 
             #seasonality_mode='multiplicative', 
             yearly.seasonality=TRUE, 
             weekly.seasonality=TRUE, 
             daily.seasonality=FALSE)


m_h_r <- add_regressor(m_h_r, 'promo_01')
m_h_r <- add_regressor(m_h_r, "promo_02")
m_h_r <- add_regressor(m_h_r, 'promo_03')
m_h_r <- add_regressor(m_h_r, "promo_04")
m_h_r <- add_regressor(m_h_r, 'promo_05')
m_h_r <- add_regressor(m_h_r, "Joghurt")
m_h_r <- add_regressor(m_h_r, "gesund.essen")
m_h_r <- fit.prophet(m_h_r, training)
```

do a forecast for the model with holydays and regressors
```{r prophet_with_holydays_and_regressors_prediction, cache=TRUE}
future <- make_future_dataframe(m_h_r, periods = ntest, freq = 60 *60, include_history = TRUE)

future$promo_01<- AL[,c("promo_01")]
future$promo_02<- AL[,c("promo_02")]
future$promo_03<- AL[,c("promo_03")]
future$promo_04<- AL[,c("promo_04")]
future$promo_05<- AL[,c("promo_05")]
future$Joghurt<- AL[,c("Joghurt")]
future$gesund.essen<- AL[,c("gesund.essen")]

fcst_h_r <- predict(m_h_r, future)
```

First visual inspection
```{r prophet_with_holydays_and_regressors_visual}
plot(m_h_r, fcst_h_r) + add_changepoints_to_plot(m_h_r)
p <- prophet_plot_components(m_h_r,fcst_h_r, render_plot = TRUE)
```

Check the accuracy of the model
```{r prophet_with_holydays_and_regressors_prediction_accuracy}
n<-nrow(training)

training$yhat_h_r<-fcst_h_r$yhat[1:n]
testing$yhat_h_r<-fcst_h_r$yhat[(n+1):(n+ntest)]

a<-training$yhat_h_r<5
training$yhat_h_r[a]<-5

b<-testing$yhat_h_r<5
testing$yhat_h_r[b]<-5

c(rmse(training$y, training$yhat_h_r),rmse(testing$y, testing$yhat_h_r))
c(mape(training$y, training$yhat_h_r),mape(testing$y, testing$yhat_h_r))
```
```{r rename_predictions_for_plot}
training<-training %>% rename(pred_prophet = yhat, pred_prophet_regressors = yhat_h_r )
testing<-testing %>% rename(pred_prophet = yhat, pred_prophet_regressors = yhat_h_r )
```
Plot the four models against eachother and add RMSE and MAPE
```{r plot_and_compare_models, warning=FALSE ,message=FALSE}
testqstat<- data.table::melt(testing, id="ds", measure=c("xgboost", 
                                                         "pred_prophet_regressors", 
                                                         "pred_glm", 
                                                         "pred_prophet"))

model_names<- c(
  'xgboost' = sprintf("Tree Boosting \n RMSE: %.2f (Train) - %.2f (Test)",
                   rmse(training$y, training$xgboost),rmse(testing$y, testing$xgboost)),
  
  'pred_prophet_regressors' = sprintf(
    "Prophet with Regressors \n RMSE: %.2f (Train) - %.2f (Test)",
                   rmse(training$y, training$pred_prophet_regressors),
                    rmse(testing$y, testing$pred_prophet_regressors)),
  
  'pred_glm' = sprintf("GLM \n  RMSE: %.2f (Train) - %.2f (Test)",
                   rmse(training$y, training$pred_glm),rmse(testing$y, testing$pred_glm)),  
  
  'pred_prophet' = sprintf("pred_prophet \n RMSE: %.2f (Train) - %.2f (Test)",
                   rmse(training$y, training$pred_prophet),rmse(testing$y, testing$pred_prophet))
)

g1 <- ggplot(aes(y=value, x=ds, color=variable), data=testqstat) +
  
  xlim(min(testing$ds), max(testing$ds))+
  geom_line(data = testing, aes(y=y), color="grey70") +
  geom_line() +
  facet_wrap(~variable, ncol=2, labeller = as_labeller(model_names))+
  theme(axis.text = element_text(size=5), axis.title = element_text(size=6)) +
  labs(title="Modelcomparison, Test Data, with rmse and mape Statistics.", 
       x="Time", y="WaitingTime in min")+ 
  theme(legend.position = "none")
g1

```